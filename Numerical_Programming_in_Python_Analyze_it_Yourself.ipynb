{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/virguleria/python-assignment/blob/main/Numerical_Programming_in_Python_Analyze_it_Yourself.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Problem Statement: Navigating the Data Science Job Landscape**\n",
        "\n",
        "üöÄ Unleash your creativity in crafting a solution that taps into the heartbeat of the data science job market! Envision an ingenious project that seamlessly wields cutting-edge web scraping techniques and illuminating data analysis.\n",
        "\n",
        "üîç Your mission? To engineer a tool that effortlessly gathers job listings from a multitude of online sources, extracting pivotal nuggets such as job descriptions, qualifications, locations, and salaries.\n",
        "\n",
        "üß© However, the true puzzle lies in deciphering this trove of data. Can your solution discern patterns that spotlight the most coveted skills? Are there threads connecting job types to compensation packages? How might it predict shifts in industry demand?\n",
        "\n",
        "üéØ The core objectives of this challenge are as follows:\n",
        "\n",
        "1. Web Scraping Mastery: Forge an adaptable and potent web scraping mechanism. Your creation should adeptly harvest data science job postings from a diverse array of online platforms. Be ready to navigate evolving website structures and process hefty data loads.\n",
        "\n",
        "2. Data Symphony: Skillfully distill vital insights from the harvested job listings. Extract and cleanse critical information like job titles, company names, descriptions, qualifications, salaries, locations, and deadlines. Think data refinement and organization.\n",
        "\n",
        "3. Market Wizardry: Conjure up analytical tools that conjure meaningful revelations from the gathered data. Dive into the abyss of job demand trends, geographic distribution, salary variations tied to experience and location, favored qualifications, and emerging skill demands.\n",
        "\n",
        "4. Visual Magic: Weave a tapestry of visualization magic. Design captivating charts, graphs, and visual representations that paint a crystal-clear picture of the analyzed data. Make these visuals the compass that guides users through job market intricacies.\n",
        "\n",
        "üåê While the web scraping universe is yours to explore, consider these platforms as potential stomping grounds:\n",
        "\n",
        "* LinkedIn Jobs\n",
        "* Indeed\n",
        "* Naukri\n",
        "* Glassdoor\n",
        "* AngelList\n",
        "\n",
        "üéà Your solution should not only decode the data science job realm but also empower professionals, job seekers, and recruiters to harness the dynamic shifts of the industry. The path is open, the challenge beckons ‚Äì are you ready to embark on this exciting journey?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WC4RKKoko7qr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GitHub Link -"
      ],
      "metadata": {
        "id": "bNJmGkF_dD9D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqcwJ-eXozg2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Project Summary:"
      ],
      "metadata": {
        "id": "psQOidhZa7Gk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Python script is a web scraping project that automates the collection of job listings for the position of \"Python Developer\" from the TimesJobs website. The project demonstrates how to use the requests library to send HTTP GET requests, BeautifulSoup for parsing HTML content, and pandas to organize the extracted data into a structured format. It iterates through multiple search result pages and extracts key details from each job listing, including the company name, required skills, years of experience, location, job description, and a direct link to the job posting. The collected data is then stored in a Pandas DataFrame and saved to a CSV file for further analysis or reference."
      ],
      "metadata": {
        "id": "OkZsKp7ja-8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Explanation:"
      ],
      "metadata": {
        "id": "qQOJTzMLbB5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Python web scraping project is designed to automate the process of gathering job listings for the position of \"Python Developer\" from the TimesJobs website. The project demonstrates the step-by-step process of collecting data from a web source using various Python libraries.\n",
        "\n",
        "Import Necessary Libraries:\n",
        "\n",
        "The script begins by importing the required Python libraries: requests, BeautifulSoup, and pandas.\n",
        "Set the Base URL and Search Parameters:\n",
        "\n",
        "The base URL for the TimesJobs website is defined as \"https://www.timesjobs.com/candidate/job-search.html.\"\n",
        "A dictionary named \"parameters\" is used to store the search parameters. These parameters include search type, search keywords (e.g., \"Python Developer\"), location (e.g., \"India\"), and the initial page number (startPage).\n",
        "Create an Empty List for Job Data:\n",
        "\n",
        "An empty list named \"jobs_data1\" is created to store the scraped job data.\n",
        "Scrape Data from Multiple Pages:\n",
        "\n",
        "The script enters a loop to scrape data from multiple search result pages. It continues scraping until the \"startPage\" parameter exceeds the \"sequence\" parameter, which determines the number of pages to scrape.\n",
        "Send an HTTP GET Request:\n",
        "\n",
        "Inside the loop, an HTTP GET request is sent to the TimesJobs website with the specified parameters using the requests library.\n",
        "Parse HTML Content:\n",
        "\n",
        "The HTML content of the response is parsed using BeautifulSoup with the 'lxml' parser.\n",
        "Find Job Listings:\n",
        "\n",
        "Job listings are identified by finding HTML elements with the class 'clearfix job-bx wht-shd-bx.'\n",
        "Extract Job Data:\n",
        "\n",
        "For each job listing, relevant information is extracted and stored in a dictionary named \"data,\" including the company name, required skills, years of experience, location(s), job description, and a link to the job posting.\n",
        "Append Data to the List:\n",
        "\n",
        "The \"data\" dictionary is appended to the \"jobs_data1\" list for each job listing found on the page.\n",
        "Increment the Page Parameter:\n",
        "\n",
        "The \"startPage\" parameter is incremented to move to the next page of search results.\n",
        "Create a Pandas DataFrame:\n",
        "\n",
        "Once all the job data has been collected, a Pandas DataFrame is created from the \"jobs_data1\" list.\n",
        "Save Data to a CSV File:\n",
        "\n",
        "The DataFrame is saved to a CSV file named \"job_listings10.csv,\" excluding the index column.\n",
        "This project demonstrates how to scrape job listings from a website, parse the HTML content, extract specific data, and store it in a structured format using Python. It showcases a practical use case of web scraping for job-related information and provides a foundation for similar data extraction tasks from other websites.\n",
        "\n",
        "Implementin web scrapping on Timesjobs.com"
      ],
      "metadata": {
        "id": "pXZsV5-ObHWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_jobs(url, keywords, location, sequence, output_filename):\n",
        "    # Define search parameters for TimesJobs website\n",
        "    search_params = {\n",
        "        'searchType': 'personalizedSearch',\n",
        "        'from': 'submit',\n",
        "        'luceneResultSize': 50,  # Number of items per page\n",
        "        'txtKeywords': keywords,  # Keywords to search for\n",
        "        'txtLocation': location,  # Location for the search\n",
        "        'sequence': sequence,  # Number of pages to scrape\n",
        "        'startPage': 1  # Start page for scraping\n",
        "    }\n",
        "\n",
        "    jobs_data = []  # Initialize an empty list to store job data\n",
        "\n",
        "    while search_params['startPage'] <= search_params['sequence']:\n",
        "        # Send an HTTP GET request to the TimesJobs website with the search parameters\n",
        "        response = requests.get(url, params=search_params)\n",
        "        soup = BeautifulSoup(response.text, 'lxml')  # Parse the response with BeautifulSoup\n",
        "        jobs = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')  # Find job listings on the page\n",
        "\n",
        "        if not jobs:\n",
        "            break  # If no job listings are found, exit the loop\n",
        "\n",
        "        for job in jobs:\n",
        "            data = {}  # Create a dictionary to store job data\n",
        "            data['Company'] = job.find('h3', class_='joblist-comp-name').get_text(strip=True)  # Extract company name\n",
        "            data['Skills'] = job.find('span', class_='srp-skills').get_text(strip=True)  # Extract required skills\n",
        "\n",
        "            ul = job.find('ul', class_='top-jd-dtl clearfix').findChildren(recursive=False)\n",
        "            data['Exp'] = ul[0].find(text=True, recursive=False)  # Extract job experience\n",
        "\n",
        "            data['Location(s)'] = ul[1].span.text if ul[1].span else None  # Extract job location(s)\n",
        "\n",
        "            ul1 = job.find('ul', class_='list-job-dtl clearfix').findChildren(recursive=False)\n",
        "            data['Desc'] = ul1[0].find('label').next_sibling.strip()  # Extract job description\n",
        "\n",
        "            data['link'] = job.header.h2.a['href']  # Extract job link\n",
        "            jobs_data.append(data)  # Append the extracted job data to the list\n",
        "\n",
        "        search_params['startPage'] += 1  # Increment the startPage parameter to scrape the next page\n",
        "\n",
        "    df = pd.DataFrame(jobs_data)  # Create a Pandas DataFrame from the scraped job data\n",
        "\n",
        "    # Save the DataFrame to a CSV file, excluding the index column\n",
        "    df.to_csv(output_filename, index=False)\n",
        "\n",
        "    return df  # Return the DataFrame containing the scraped job data"
      ],
      "metadata": {
        "id": "CrE73N4_bMa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling the function to scrape jobs and save to a CSV file\n",
        "df = scrape_jobs(\"https://www.timesjobs.com/candidate/job-search.html\", \"Python Developer\", \"India\", 3, \"job_listings10.csv\")"
      ],
      "metadata": {
        "id": "vks7osUObOn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "aPOHJsw4bQHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets check the summary\n",
        "df.info()"
      ],
      "metadata": {
        "id": "YYCfCHw6bRr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data cleanning"
      ],
      "metadata": {
        "id": "c_oKdfkSbTZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Checking Null Values if Any"
      ],
      "metadata": {
        "id": "rKU_KEVrbWTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "ErhXTUHqbeNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####No Null values present"
      ],
      "metadata": {
        "id": "5Md4qGj3bgWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####There are multiple Job positions with the name of same company. That will count as duplicates , so we will be avoid dropping them"
      ],
      "metadata": {
        "id": "YEoCfTyhbjtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cleaning and formatting the 'Exp' column to extract numeric experience values."
      ],
      "metadata": {
        "id": "M9m87v4xbv5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and format the 'Exp' column\n",
        "df['Exp'] = df['Exp'].str.replace(' yrs', '').str.replace(' Yrs', '').str.replace('yr', '').str.replace('Yr', '').str.strip()\n",
        "df['Exp'] = df['Exp'].str.extract(r'(\\d+)').astype(float)"
      ],
      "metadata": {
        "id": "v3eaSkxwbyl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cleaning and formatting the 'Location(s)' column to remove extra spaces."
      ],
      "metadata": {
        "id": "bYlvrxYJb0Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and format the 'Skills' column by stripping extra whitespace\n",
        "df['Skills'] = df['Skills'].str.strip()"
      ],
      "metadata": {
        "id": "inql_jx0b3Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(n = 20 )"
      ],
      "metadata": {
        "id": "vtEeYn5Qb40x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Data is Cleaned now lets do some visualizations"
      ],
      "metadata": {
        "id": "U0xX75n_b63z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Visualization"
      ],
      "metadata": {
        "id": "eUlel5xyb-vG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Count of Job Listings by Company"
      ],
      "metadata": {
        "id": "SGYbgq7CcDU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count job listings by company\n",
        "company_counts = df['Company'].value_counts()\n",
        "\n",
        "# Plot the top 10 companies with the most job listings\n",
        "top_10_companies = company_counts.head(10)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_10_companies.plot(kind='bar')\n",
        "plt.title('Top 10 Companies with Most Job Listings')\n",
        "plt.xlabel('Company')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T6KVVbxLcGgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observations::"
      ],
      "metadata": {
        "id": "yGrAnfBocIzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. As you can see Anicalls pty ltd and Tanda HR solutions Hire very agressively .\n",
        "2. They look like growing companie"
      ],
      "metadata": {
        "id": "y5qcls2fcLu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Job Experience Distribution"
      ],
      "metadata": {
        "id": "6S05qY2QcPRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Plot the distribution of job experience levels\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, x='Exp')\n",
        "plt.title('Job Experience Distribution')\n",
        "plt.xlabel('Experience Level')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tlguKlyUcRwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Observation:"
      ],
      "metadata": {
        "id": "8baW9ng0cTiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. There are plenty amount of jobs For Experience Python Developer In India.\n",
        "2. Even Freshers with 0 Experience have few jobs as well."
      ],
      "metadata": {
        "id": "COaMzo5YcWFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Skills Required"
      ],
      "metadata": {
        "id": "122L-au8cZe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the most common skills required\n",
        "skills_counts = df['Skills'].str.split(', ').explode().value_counts()\n",
        "\n",
        "# Plot the top 10 skills\n",
        "top_10_skills = skills_counts.head(10)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_10_skills.plot(kind='bar')\n",
        "plt.title('Top 10 Required Skills')\n",
        "plt.xlabel('Skill')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E2aIFl7gccaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Observations:"
      ],
      "metadata": {
        "id": "QLp2S3zRcfLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Python developer jobs these You need to have knowledge about django , python , rest , nosql , docker technologies because these are in demand skills and companies are hiring for these technology experts."
      ],
      "metadata": {
        "id": "YkPnf_mCchqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Bar Plot of Top 10 Locations with Most Job Listings"
      ],
      "metadata": {
        "id": "uqXkyL4AcjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count job listings by location\n",
        "location_counts = df['Location(s)'].value_counts()\n",
        "\n",
        "# Plot the top 10 locations with the most job listings\n",
        "top_10_locations = location_counts.head(10)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_10_locations.plot(kind='bar')\n",
        "plt.title('Top 10 Locations with Most Job Listings')\n",
        "plt.xlabel('Location')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZX_sZnLkcltp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observation:"
      ],
      "metadata": {
        "id": "LM_vBkgscnhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Looking at Job locations most of these companies hire for Bengluru location.\n",
        "2. Many of them hire for Hydrabad and Chandigarh Locations.\n",
        "3. As expected because these are IT hubs of Our India."
      ],
      "metadata": {
        "id": "CfLInM0Ecp1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Word Cloud of Job Descriptions"
      ],
      "metadata": {
        "id": "3oY0up-Rcv50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all job descriptions into a single string\n",
        "job_descriptions = \" \".join(df['Desc'])\n",
        "\n",
        "# Create a word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(job_descriptions)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title('Word Cloud of Job Descriptions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p4d2rka0czLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Observation:"
      ],
      "metadata": {
        "id": "tB7RB3B8c1D9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. This word cloud displays the most common words used in job descriptions, where the size of each word corresponds to its frequency.\n",
        "2. If you add these words in your resume , there is really high chance you getting shortlisted through their ATS."
      ],
      "metadata": {
        "id": "D2Tgnld-c3an"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion:"
      ],
      "metadata": {
        "id": "DudNOTYdc65K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This web scraping project provides a practical example of how to collect job-related information from the TimesJobs website using Python. It demonstrates the use of popular libraries like requests, BeautifulSoup, and pandas to automate the data extraction process. The script can be modified to scrape job listings for different positions, locations, or additional details, making it a versatile tool for gathering data from online job portals. The extracted data is organized in a structured format, making it easy for further analysis or reference."
      ],
      "metadata": {
        "id": "LYRD-e6pc98t"
      }
    }
  ]
}